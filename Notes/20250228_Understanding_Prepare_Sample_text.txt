Summary of prepare_sample_text (Detailed Breakdown)
The function prepare_sample_text is responsible for tokenizing and structuring input/output text for the model.
It ensures that both input (RISC-V) and output (ARM) sequences are properly formatted, while also tracking key indices for alignment.

1️⃣ Function Signature
python
Copy
Edit
def prepare_sample_text(tokenizer, example, input_lang, output_lang, as_text=False)
Parameter	Purpose
tokenizer	Tokenizer from Hugging Face (used to convert text into token IDs).
example	A dictionary containing example[input_lang] (RISC-V) and example[output_lang] (ARM).
input_lang	Language code for input (e.g., "risc").
output_lang	Language code for output (e.g., "arm").
as_text	If True, return raw text instead of tokenized tensors (used for debugging).
2️⃣ Step-by-Step Execution
🔹 Step 1: Handle Debugging Mode
python
Copy
Edit
if as_text:
    return f"<{input_lang}>:{example[input_lang]}\n\n<{output_lang}>:{example[output_lang]}"
✅ If as_text=True, return the raw input/output text as a formatted string (useful for debugging).
🚀 Otherwise, continue with tokenization.

🔹 Step 2: Tokenize the Input Language Prefix (<RISC>:)
python
Copy
Edit
input_ids = tokenizer(f"<{input_lang}>:", return_tensors='pt').input_ids
if input_ids[0][-1] == tokenizer.eos_token_id:
    input_ids = input_ids[:,:-1]  # Remove EOS token if it exists
✅ Tokenizes <RISC>: so the model knows which language to expect.
📌 Removes EOS (</s>) token if present to prevent unnecessary tokens.

🔹 Step 3: Store Where Input Starts
python
Copy
Edit
in_start_idx = input_ids.shape[-1]
✅ Stores the position where the actual program starts after <RISC>:.
🚀 This helps align tokenized inputs properly.

🔹 Step 4: Tokenize the Actual Input Code
python
Copy
Edit
input_seq_tokenized = tokenizer(example[input_lang], return_tensors='pt').input_ids
if input_seq_tokenized[0][0] == tokenizer.bos_token_id:
    input_seq_tokenized = input_seq_tokenized[:, 1:]  # Remove BOS token
✅ Tokenizes the RISC-V code from example[input_lang].
📌 Removes BOS (<s>) if it exists to keep a clean sequence.

🔹 Step 5: Store Input Sequence Length
python
Copy
Edit
in_seq_len = input_seq_tokenized.shape[-1]
✅ Tracks how long the input sequence (RISC-V function) is.
🚀 Needed for proper slicing and alignment.

🔹 Step 6: Tokenize the Target Language Prefix ("\n\n<ARM>:")
python
Copy
Edit
tgt_lang_prefix = tokenizer(f"\n\n<{output_lang}>:", return_tensors='pt').input_ids
if tgt_lang_prefix[0][-1] == tokenizer.eos_token_id:
    tgt_lang_prefix = tgt_lang_prefix[:,:-1]  # Remove EOS token if present
if tgt_lang_prefix[0][0] == tokenizer.bos_token_id:
    tgt_lang_prefix = tgt_lang_prefix[:, 1:]  # Remove BOS token if present
✅ Tokenizes "\n\n<ARM>:" to mark where the translation starts.
📌 Removes BOS/EOS if present to ensure clean formatting.

🔹 Step 7: Concatenate Everything (Full Input)
python
Copy
Edit
input_ids = torch.cat((input_ids, input_seq_tokenized, tgt_lang_prefix), dim=-1)
✅ Combines everything into a single tensor:

<RISC>: (Prefix)
RISC-V Code (Input Function)
"\n\n<ARM>:" (Target Prefix)
🔹 Step 8: Store Where the Output Starts
python
Copy
Edit
out_start_idx = input_ids.shape[-1]
✅ Stores the index where the actual output (translated ARM assembly) begins.
🚀 This helps align the model’s attention mechanisms.

🔹 Step 9: Tokenize the Target Output Code
python
Copy
Edit
output_seq_tokenized = tokenizer(example[output_lang], return_tensors='pt').input_ids
if output_seq_tokenized[0][0] == tokenizer.bos_token_id:
    output_seq_tokenized = output_seq_tokenized[:, 1:]  # Remove BOS token
✅ Tokenizes the translated ARM code from example[output_lang].
📌 Removes BOS (<s>) if present to keep things clean.

🔹 Step 10: Store Output Sequence Length
python
Copy
Edit
out_seq_len = output_seq_tokenized.shape[-1]
✅ Stores how many tokens are in the translated ARM function.
🚀 Helps track sequence alignment.

🔹 Step 11: Concatenate Everything (Final Model Input)
python
Copy
Edit
input_ids = torch.cat((input_ids, output_seq_tokenized), dim=-1)
✅ Now input_ids is fully prepared and contains:

<RISC>: Prefix
RISC-V Input Function
"\n\n<ARM>:" Target Prefix
ARM Translated Function
🔹 Step 12: Return Values
python
Copy
Edit
return input_ids, (in_start_idx, in_seq_len, out_start_idx, out_seq_len)
✅ Returns:

input_ids → The fully tokenized sequence (prefix + input + target + output).
Tuple (in_start_idx, in_seq_len, out_start_idx, out_seq_len) → These values track the boundaries for later processing.
📌 Final Summary
🛠 What Does prepare_sample_text Do?
Tokenizes everything (prefixes + input + output).
Tracks key indices to align input & output sequences.
Ensures BOS/EOS tokens are removed for clean formatting.
Returns a structured tensor that can be fed into the model.



_______________________ATTENTION SCORES__________________________

📌 How Are Attention Scores Calculated?
Attention scores in a transformer model are computed using the scaled dot-product attention mechanism. The process follows these key steps:

1️⃣ Computing Attention Scores: Scaled Dot-Product Attention
For each token in the input sequence, the model calculates how much attention it should give to all other tokens using the formula:

Attention
(
𝑄
,
𝐾
,
𝑉
)
=
softmax
(
𝑄
𝐾
𝑇
𝑑
𝑘
)
𝑉
Attention(Q,K,V)=softmax( 
d 
k
​
 
​
 
QK 
T
 
​
 )V
Where:

𝑄
Q (Query): Represents the current token.
𝐾
K (Key): Represents all tokens in the sequence.
𝑉
V (Value): Represents the actual word embeddings.
𝑑
𝑘
d 
k
​
  is the dimensionality of the key vectors.
Softmax ensures attention weights sum to 1.
✅ This tells the model which tokens are most relevant for generating the next output token.

2️⃣ What Do Attention Scores Represent at Each Step?
Each step in sequence generation updates the attention scores, allowing the model to determine which parts of the input are most important.

🟢 Example: Translating RISC-V to ARM
Step	Current Token	Top Attended Tokens
Step 1	"addi"	"addi", "a0"
Step 2	"a0"	"a0", "addi"
Step 3	","	"a0", "addi"
Step 4	"a0"	"a0", "1"
Step 5	"1"	"1", "addi"
🔹 What This Means:

At each step, the model computes attention scores dynamically.
Early steps focus on understanding the instruction.
Later steps adjust focus toward generating correct translations.
🚀 The attention mechanism allows the model to selectively “look back” at the most relevant parts of the input while generating output.

3️⃣ How Are Attention Scores Used in This Project?
📌 In standard transformers, attention scores are only used internally.
📌 In this research, attention matrices are extracted and analyzed to understand alignment.

✅ We compute:

Which input tokens influence each output token.
How strongly different parts of the input contribute to the translation.
Whether model predictions align with expected translations.
📌 Final Takeaways
Attention scores are computed dynamically at each generation step.
They tell the model which input tokens to focus on while generating output.
This project extracts and analyzes attention matrices to measure alignment between source and translated code.




_________________________________PREPROCESSING SUMMARY_________________

Summary of preprocess_text
The preprocess_text function prepares input text for the model, ensuring that it is correctly formatted for both encoder-decoder models (e.g., BART, T5) and decoder-only models (e.g., GPT, LLaMA).

1️⃣ If the Model is Encoder-Decoder (is_enc_dec=True)
python
Copy
Edit
if self.is_enc_dec:
    model_inputs = self.tokenizer([input_text], return_tensors="pt")
    if tgt_text is not None:
        labels = self.tokenizer(text_target=[tgt_text], return_tensors="pt")
        model_inputs["labels"] = labels["input_ids"]
    return model_inputs
✅ What Happens?

Tokenizes input_text → Stores as "input_ids".
Tokenizes tgt_text separately → Stores as "labels".
Returns model_inputs with both input & expected output.
🔹 Why?

Encoder-decoder models expect input and output as separate sequences.
The "labels" field is needed for training.
2️⃣ If the Model is Decoder-Only (is_enc_dec=False)
python
Copy
Edit
else:
    input_ids, (in_start_idx, in_seq_len, out_start_idx, out_seq_len) = prepare_sample_text(
        self.tokenizer, {self.src_lang: input_text, self.tgt_lang: tgt_text}, self.src_lang, self.tgt_lang
    )
✅ What Happens?

Calls prepare_sample_text, which:
Tokenizes input and output together as one sequence.
Extracts indices (in_start_idx, out_start_idx) to track boundaries.
🔹 Why?

Decoder-only models need everything in one sequence.
The model learns by predicting the output tokens after seeing the input.
3️⃣ Structuring model_inputs for Decoder-Only Models
python
Copy
Edit
model_inputs = self.tokenizer(input_text)
model_inputs.input_ids = torch.tensor(input_ids[:, :out_start_idx])
model_inputs.labels = torch.tensor(input_ids)
model_inputs.attention_mask = torch.ones_like(model_inputs.input_ids)
✅ What Happens?

Creates model_inputs dictionary.
Manually sets "input_ids" to only include the input portion.
Sets "labels" to contain the full sequence (input + output).
Creates attention_mask to tell the model which tokens to focus on.
🔹 Why?

The "labels" field is used for training (so the model learns to predict the output).
The "attention_mask" ensures the model attends only to real tokens.
📌 Final Takeaways
📌 For encoder-decoder models, input and output are processed separately.
📌 For decoder-only models, everything is in one sequence, and boundaries are tracked.
📌 Attention masks help guide the model's focus.
🚀 Now the data is ready for training or inference




Aside from Qwen, is the code the same as before (aside from trivial formatting)?
✅ Yes, the code is structurally the same except for the Qwen addition.

Encoder-decoder models (is_enc_dec=True) remain unchanged.
Decoder-only models (is_enc_dec=False) still use prepare_sample_text to process input-output sequences.
Formatting changes are minimal (e.g., better readability, structured input).
🚀 Conclusion:
✅ The logic for standard encoder-decoder and decoder-only models remains identical.

2️⃣ Is Qwen Handled Accurately?
📌 What’s Different for Qwen?
python
Copy
Edit
elif "qwen" in self.args.model_name_or_path.lower():
    # Use a natural language prompt instead of structured format. Need prompt to work
    formatted_input = f"Translate the following {self.src_lang} code to {self.tgt_lang}:\n{input_text}\n"
    formatted_target = f"{tgt_text}" if tgt_text is not None else None
    model_inputs = self.tokenizer(formatted_input, return_tensors="pt")
    if formatted_target:
        labels = self.tokenizer(formatted_target, return_tensors="pt")
        model_inputs["labels"] = labels["input_ids"]
    return model_inputs
✅ Why is Qwen handled differently?

Natural Language Prompting

Instead of using structured prefixes like <RISC>: ... <ARM>: ..., Qwen expects an explicit natural language prompt.
"Translate the following {src_lang} code to {tgt_lang}:\n{input_text}\n"
✅ This makes sense because Qwen models (especially Qwen2.5) are trained to follow natural prompts rather than structured token prefixes.
Formatting Target Text

python
Copy
Edit
formatted_target = f"{tgt_text}" if tgt_text is not None else None
Keeps the target text untouched while maintaining compatibility. ✅ Correct because we don’t need special delimiters for Qwen’s output.
Tokenization & Labels

python
Copy
Edit
model_inputs = self.tokenizer(formatted_input, return_tensors="pt")
if formatted_target:
    labels = self.tokenizer(formatted_target, return_tensors="pt")
    model_inputs["labels"] = labels["input_ids"]
Tokenizes the prompt-style input instead of a structured sequence.
Handles labels in the same way as encoder-decoder models (optional target).
✅ Correct because Qwen’s generation process requires this format.
🚀 Final Verdict
✅ Qwen is handled accurately because:

It adapts to natural language prompting (as required by Qwen models).
It follows the same logic as encoder-decoder models but with a different structure.
It maintains the standard format for tokenization and labels.

